{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis\n",
    "from sklearn.metrics import log_loss\n",
    "import copy\n",
    "import scipy.stats\n",
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dark background style\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CSV\n",
    "df = pd.read_csv('../data/data_v3.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "target = 'fraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning\n",
    "train = df[df[target].isnull() == False]\n",
    "test  = df[df[target].isnull() == True]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "y = train[target]\n",
    "del train[target], test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop bad features\n",
    "excluded_feats = ['id']\n",
    "features = [f for f in train.columns if f not in excluded_feats]\n",
    "print(train[features].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynozDG6yivwQ"
   },
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "\n",
    "# settings\n",
    "cores = 12\n",
    "seed  = 23\n",
    "\n",
    "# cross-validation\n",
    "num_folds = 5\n",
    "shuffle   = True\n",
    "\n",
    "# muner of rounds\n",
    "max_rounds = 600\n",
    "stopping   = 600\n",
    "verbose    = 200\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    #'objective':          custom_loss,\n",
    "    #'objective':          'binary:hinge',\n",
    "    #'objective':          'binary:logistic',\n",
    "    #'boosting_type':     'gbdt',\n",
    "    #'objective':         'binary',\n",
    "    #'metrics':           'logloss',\n",
    "    'eval_metric':       'logloss',\n",
    "    'n_estimators' :     120,#[int(i) for i in np.arange(20,200.001,20)], #150,\n",
    "    #'bagging_fraction':  0.9,\n",
    "    #'feature_fraction':  0.8,\n",
    "    'subsample' :        1,#[i for i in np.arange(0.8,1.01,0.025)],#1,\n",
    "    #'alpha':             [10**i for i in np.arange(0,8.001,1)],#0.1,\n",
    "    #'lambda':            [2**i for i in np.arange(0,11,1)],#0,\n",
    "    #'eta':               [10**i for i in np.arange(0,11,1)],\n",
    "    #'min_split_gain':    0.01,\n",
    "    'min_child_weight':  0.85,#[i for i in np.arange(0.6,1.01,0.05)] + [i for i in np.arange(0.8,0.90001,0.01)], #0.85,\n",
    "    #'min_child_samples': 20,\n",
    "\n",
    "    'silent':            True,\n",
    "    'verbosity':         -1,\n",
    "    'learning_rate':     0.31,#0.31,#[i for i in np.arange(0.2,0.5,0.01)],#0.25,\n",
    "    'max_depth':         6,\n",
    "    #'max_leaves':        [i for i in np.arange(0,10,1)],#70,\n",
    "    #'scale_pos_weight':  1,\n",
    "    #'n_estimators':      max_rounds,\n",
    "    #'nthread' :          cores,\n",
    "    #'random_state':      seed,\n",
    "    'colsample_bytree':  1,#[i for i in np.arange(0.5,1.0001,0.05)], \n",
    "    'colsample_bylevel': 1,#[i for i in np.arange(0.5,1.0001,0.025)],#0.9,#[i for i in np.arange(0.5,1.0001,0.05)], \n",
    "    'colsample_bynode':  0.6#[i for i in np.arange(0.5,0.65,0.01)]#[i for i in np.arange(0.25,0.75,0.01) if i >=0.36 and i<=0.47 or i >=0.54 and i<=0.64]#0.65,#[i for i in np.arange(0.5,1.0001,0.05)]\n",
    "}\n",
    "# data partitinoing\n",
    "folds = StratifiedKFold(n_splits = num_folds, random_state = seed, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "clfs = []\n",
    "valid_profit = np.zeros(num_folds) \n",
    "preds_test   = np.zeros(test.shape[0])\n",
    "preds_oof    = np.zeros(train.shape[0])\n",
    "importances  = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE settings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "sm = SMOTE(random_state = 23, n_jobs = 10, sampling_strategy = 0.1)\n",
    "sm2 = SMOTEENN(smote=sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from collections.abc import Iterable\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [0,100]\n",
    "scaler_params = {\n",
    "    #'quantile_range': [(qs[0],i) for i in np.arange(0,101) if i > qs[0]],\n",
    "    #'quantile_range': [(i,qs[1]) for i in np.arange(0,101,20) if i < qs[1]],\n",
    "    #'quantile_range': [(qs[0],i) for i in np.arange(qs[1]-5,qs[1]+5,0.1) if i > qs[0]],\n",
    "    #'quantile_range': [(i,qs[1]) for i in np.arange(qs[0]-5,qs[0]+5,0.1) if i < qs[1]],\n",
    "    #'with_centering': [True,False],\n",
    "#    'with_scaling': [True,False]\n",
    "}\n",
    "params = dict()\n",
    "params.update({'clf__'+k: v if isinstance(v,Iterable) and not isinstance(v,str) else [v] for k,v in lgb_params.items()})\n",
    "#params.update({'rs__'+k:   v if isinstance(v,Iterable) else [v] for k,v in scaler_params.items()})\n",
    "np.array([len(v) for k,v in params.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params['clf__objective'] = [custom_loss3]\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator = Pipeline([('rs', RobustScaler()),('sm',sm2),('clf',XGBClassifier())]),\n",
    "    n_iter = 2000,\n",
    "    param_distributions = params,\n",
    "    cv=num_folds, \n",
    "    n_jobs=cores,\n",
    "    verbose=2)\n",
    "rs.fit(train[features],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_star = {k.replace('lgbmclassifier__',''):v for k,v in rs.best_params_.items() if 'lgbmclassifier' in k}\n",
    "print(rs.best_score_)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rs.cv_results_['param_clf__n_estimators'].data,rs.cv_results_['mean_test_score'])\n",
    "#m = rs.cv_results_['mean_test_score'].max()\n",
    "#pd.DataFrame(rs.cv_results_['param_clf__min_child_weight'].data[rs.cv_results_['mean_test_score'] >= m]).round(3)\n",
    "#a = np.arange(0.37,0.8,0.01)\n",
    "#rs.cv_results_['param_clf__colsample_bynode'].data[rs.cv_results_['mean_test_score'] >= m].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(rs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(rs.estimator.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in rs.estimator.get_params().keys() if 'eval' in i]\n",
    "params_tmp = [i.replace('clf__','') for i in rs.estimator.get_params().keys() if 'clf__' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CROSS-VALIDATION LOOP\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, y)):\n",
    "    \n",
    "    # data partitioning\n",
    "    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # scale data\n",
    "    scaler   = RobustScaler()\n",
    "    trn_x    = pd.DataFrame(scaler.fit_transform(trn_x),      columns = features)\n",
    "    val_x    = pd.DataFrame(scaler.transform(val_x),          columns = features)\n",
    "    tmp_test = pd.DataFrame(scaler.transform(test[features]), columns = features)\n",
    "\n",
    "    # augment training data with SMOTE\n",
    "    trn_x, trn_y = sm.fit_sample(trn_x, trn_y)\n",
    "    trn_x = pd.DataFrame(trn_x, columns = features)\n",
    "    trn_y = pd.Series(trn_y)\n",
    "    \n",
    "    # factor decomposition\n",
    "    tmp_features = copy.deepcopy(features)\n",
    "    \n",
    "    # train lightGBM\n",
    "    print('Custom early stopping: select the best out of %.0f iterations...' % max_rounds)\n",
    "    clf = rs.best_estimator_.get_params()['clf']\n",
    "    #clf = XGBClassifier()\n",
    "    clf = clf.fit(trn_x, trn_y, \n",
    "                  eval_set              = [(trn_x, trn_y), (val_x, val_y)], \n",
    "                  eval_metric           = prediction_reward_xgb, \n",
    "                  #eval_metric           = \"logloss\", \n",
    "                  #early_stopping_rounds = stopping,\n",
    "                  verbose               = 0\n",
    "                 )\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # find the best iteration\n",
    "    best_iter = np.argmax(clf.evals_result_['validation_1']['profit']) + 1\n",
    "    \n",
    "    # predictions\n",
    "    preds_oof[val_idx]    = clf.predict_proba(val_x, ntree_limit = best_iter)[:, 1]\n",
    "    valid_profit[n_fold]  = prediction_reward(val_y, preds_oof[val_idx])[1]\n",
    "    preds_test           += clf.predict_proba(tmp_test, ntree_limit = best_iter)[:, 1] / folds.n_splits \n",
    "\n",
    "    ## importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df['Feature'] = tmp_features\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold'] = n_fold + 1\n",
    "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # print performance\n",
    "    print('--------------------------------')\n",
    "    print('FOLD%2d: PROFIT = %.0f' % (n_fold + 1, valid_profit[n_fold]))\n",
    "    print('--------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    # clear memory\n",
    "    del trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()\n",
    "    \n",
    "    # uncomment for mean target encoding\n",
    "    #features = [f for f in train.columns if f not in excluded_feats]\n",
    "    \n",
    "    \n",
    "# print overall performance    \n",
    "cv_perf = np.sum(valid_profit)\n",
    "print('--------------------------------')\n",
    "print('TOTAL PROFIT = %.0f' % cv_perf)\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RECHECK PROFIT  \n",
    "prediction_reward(y, preds_oof)\n",
    "\n",
    "\n",
    "###### TRACKING RESULTS (5 folds, strat = True, seed = 23)\n",
    "\n",
    "# V1: lgb, 5 folds, default features:   80\n",
    "# V2: add feature:  no. total items:   250\n",
    "# V3: use logloss for ES, not profit:  260\n",
    "# V4: add feature: no. weird actions:  275\n",
    "# V5: custom earlystop for profit:     320\n",
    "# V6: add SMOTE for minority class:    335\n",
    "# V7: add robust data scaling:         350 = 95 + 55 + 75 + 35 + 90\n",
    "# V8: increase learning rate to 0.1:   375 = 95 + 65 + 75 + 50 + 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VARIABLE IMPORTANCE\n",
    "\n",
    "# load importance    \n",
    "top_feats = 100\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "    \n",
    "# plot variable importance\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False))\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot as pdf\n",
    "plt.savefig('../var_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUTOFF OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OPTIMIZE CUTOFF\n",
    "\n",
    "# set step\n",
    "step = 100\n",
    "\n",
    "# search\n",
    "cutoffs = []\n",
    "profits = []\n",
    "for i in range(0, step):\n",
    "    cutoffs.append(i / step)\n",
    "    profits.append(recompute_reward(y, preds_oof, cutoff = cutoffs[i]))\n",
    "        \n",
    "# results\n",
    "plt.figure(figsize = (10,4))\n",
    "sns.lineplot(x = cutoffs[10:step], y = profits[10:step], color = 'red')\n",
    "plt.tight_layout()\n",
    "plt.axvline(x = cutoffs[np.argmax(profits)], color = 'white', linestyle = '--')\n",
    "print('- optimal cutoff = %.4f' % cutoffs[np.argmax(profits)])\n",
    "print('- optimal profit = %.4f' % profits[np.argmax(profits)])\n",
    "plt.savefig('../cutoff_selection.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name\n",
    "model = 'xgb_v4'\n",
    "perf  = str(round(cv_perf, 0).astype('int'))\n",
    "name  = model + '_' + perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OOF preds\n",
    "oof = pd.DataFrame({'id': train['id'], 'fraud': preds_oof})\n",
    "oof.to_csv('../oof_preds/' + str(name) + '.csv', index = False)\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check submission\n",
    "sub = pd.DataFrame({'id': test['id'], 'fraud': preds_test})\n",
    "sub['fraud'] = np.round(sub['fraud']).astype('int')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export submission\n",
    "sub = sub[['fraud']]\n",
    "sub.to_csv('../submissions/' + str(name) + '.csv', index = False)\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation with previous submission\n",
    "prev_sub = pd.read_csv('../submissions/lgb_v8_375.csv')\n",
    "cor = np.sum(prev_sub[target] == sub.reset_index()[target]) / len(sub)\n",
    "print(\"Share of the same predictions: \" + str(np.round(cor, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
