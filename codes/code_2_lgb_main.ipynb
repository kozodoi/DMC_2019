{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDg2hoRrG4UL"
   },
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LofLYLANetP5"
   },
   "outputs": [],
   "source": [
    "# settings for Google Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "import os\n",
    "os.chdir(\"/gdrive/My Drive/Colab Notebooks/DMC_2019/codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QpPWtHZG4UM"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import copy\n",
    "import scipy.stats\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leWiYXK_G4UQ"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oh1I8GrTG4UT"
   },
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXVmT0uFG4UV"
   },
   "outputs": [],
   "source": [
    "# dark background style\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgOaoV66G4UX"
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmYH_UjRG4Ua"
   },
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NraN1BCPG4Uc"
   },
   "source": [
    "# 2. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1557933093757,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg",
      "userId": "01253993997636551956"
     },
     "user_tz": -120
    },
    "id": "7QV5eQHHG4Ud",
    "outputId": "2389881e-8c0a-4518-c185-d0a84d1d4a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 13)\n"
     ]
    }
   ],
   "source": [
    "# import CSV\n",
    "df = pd.read_csv('../data/data_v3.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfsTSMBAG4Uf"
   },
   "outputs": [],
   "source": [
    "# target variable\n",
    "target = 'fraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1557933093762,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg",
      "userId": "01253993997636551956"
     },
     "user_tz": -120
    },
    "id": "hxJatDSBG4Uh",
    "outputId": "9b7dd4d7-fbf6-493b-d23c-25fdbf58ee37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1879, 13)\n",
      "(498121, 13)\n"
     ]
    }
   ],
   "source": [
    "# partitioning\n",
    "train = df[df[target].isnull() == False]\n",
    "test  = df[df[target].isnull() == True]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1557933093763,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg",
      "userId": "01253993997636551956"
     },
     "user_tz": -120
    },
    "id": "J5B6LXZq4hu_",
    "outputId": "2202936a-cb06-40d0-9184-0807835d11fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"features_for_n = ['grandTotal',\\n 'lineItemVoids',\\n 'lineItemVoidsPerPosition',\\n 'quantityModifications',\\n 'scannedLineItemsPerSecond',\\n 'scansWithoutRegistration',\\n 'totalScanTimeInSeconds',\\n 'valuePerSecond',\\n 'total_items',\\n 'weird_actions']\\n\\ntrain['neighbors_fraud'] = 0\\ntest['neighbors_fraud'] = 0\\n\\nprint('create train neighboring features')\\nnbrs = NearestNeighbors(n_neighbors=100, algorithm='ball_tree').fit(train.loc[:,features_for_n].values)\\nfor i, obs in enumerate(train.index):\\n  distances, indices = nbrs.kneighbors(train.loc[[obs], features_for_n].values)\\n  train.loc[obs, 'neighbors_fraud'] = train.loc[indices[0][indices[0]!=obs], 'fraud'].mean()\\n  if i%100==0:\\n    print(i)\""
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create neigbhor-based featurs (by Liza)\n",
    "\n",
    "'''features_for_n = ['grandTotal',\n",
    " 'lineItemVoids',\n",
    " 'lineItemVoidsPerPosition',\n",
    " 'quantityModifications',\n",
    " 'scannedLineItemsPerSecond',\n",
    " 'scansWithoutRegistration',\n",
    " 'totalScanTimeInSeconds',\n",
    " 'valuePerSecond',\n",
    " 'total_items',\n",
    " 'weird_actions']\n",
    "\n",
    "train['neighbors_fraud'] = 0\n",
    "test['neighbors_fraud'] = 0\n",
    "\n",
    "print('create train neighboring features')\n",
    "nbrs = NearestNeighbors(n_neighbors=100, algorithm='ball_tree').fit(train.loc[:,features_for_n].values)\n",
    "for i, obs in enumerate(train.index):\n",
    "  distances, indices = nbrs.kneighbors(train.loc[[obs], features_for_n].values)\n",
    "  train.loc[obs, 'neighbors_fraud'] = train.loc[indices[0][indices[0]!=obs], 'fraud'].mean()\n",
    "  if i%100==0:\n",
    "    print(i)'''\n",
    "\n",
    "    \n",
    "    \n",
    "#print('create train neighboring features')\n",
    "#nbrs = NearestNeighbors(n_neighbors=16, algorithm='ball_tree').fit(train.loc[:,features_for_n])\n",
    "#for i, obs in enumerate(train.index):\n",
    "#  distances, indices = nbrs.kneighbors(train.loc[[obs], features_for_n])\n",
    "#  train.loc[obs, 'neighbors_fraud15'] = train.loc[indices[0][indices[0]!=obs], 'fraud'].mean()\n",
    "#  if i%100==0:\n",
    "#    print(i)\n",
    "    \n",
    "#print('create test neighboring features')  \n",
    "#nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(train.loc[:,features_for_n])\n",
    "#for i, obs in enumerate(test.index):\n",
    "#  distances, indices = nbrs.kneighbors(test.loc[[obs], features_for_n])\n",
    "#  test.loc[obs, 'neighbors_fraud'] = train.loc[indices[0], 'fraud'].mean()\n",
    "#  if i%1000==0:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMbdrUNnG4Uk"
   },
   "outputs": [],
   "source": [
    "# target variable\n",
    "y = train[target]\n",
    "del train[target], test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPX4ljoNG4Un"
   },
   "source": [
    "# 3. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B69Fdhg0G4Uo"
   },
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1557933094450,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg",
      "userId": "01253993997636551956"
     },
     "user_tz": -120
    },
    "id": "eWm2I0jDG4Uo",
    "outputId": "89ecbd36-3fac-47aa-dc7f-5b04b4d6081a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1879, 11)\n"
     ]
    }
   ],
   "source": [
    "# drop bad features\n",
    "excluded_feats = ['id']\n",
    "features = [f for f in train.columns if f not in excluded_feats]\n",
    "print(train[features].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUIqo-MmG4Uq"
   },
   "outputs": [],
   "source": [
    "# perform PCA\n",
    "#num_comp = 5\n",
    "#pca = PCA(n_components = num_comp)\n",
    "#pca.fit(train[features].values)\n",
    "#train = pd.DataFrame(pca.transform(train[features].values), columns = ['v' + str(v) for v in range(0, num_comp)])\n",
    "#test  = pd.DataFrame(pca.transform(test[features].values),  columns = ['v' + str(v) for v in range(0, num_comp)])\n",
    "#features = list(train.columns)\n",
    "#print(train[features].shape)\n",
    "#print(test[features].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynozDG6yivwQ"
   },
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "\n",
    "# settings\n",
    "cores = 12\n",
    "seed  = 23\n",
    "\n",
    "# cross-validation\n",
    "num_folds = 5\n",
    "shuffle   = True\n",
    "\n",
    "# muner of rounds\n",
    "max_rounds = 600\n",
    "stopping   = 600\n",
    "verbose    = 200\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    'boosting_type':     'gbdt',\n",
    "    'objective':         'binary',\n",
    "    'metrics':           'binary_logloss',\n",
    "    'bagging_fraction':  0.9,\n",
    "    'feature_fraction':  0.8,\n",
    "    'lambda_l1':         0.1,\n",
    "    'lambda_l2':         0.1,\n",
    "    'min_split_gain':    0.01,\n",
    "    'min_child_weight':  2,\n",
    "    'min_child_samples': 20,\n",
    "    'silent':            True,\n",
    "    'verbosity':         -1,\n",
    "    'learning_rate':     0.1,\n",
    "    'max_depth':         7,\n",
    "    'num_leaves':        70,\n",
    "    'scale_pos_weight':  1,\n",
    "    'n_estimators':      max_rounds,\n",
    "    'nthread' :          cores,\n",
    "    'random_state':      seed,\n",
    "}\n",
    "\n",
    "# data partitinoing\n",
    "folds = StratifiedKFold(n_splits = num_folds, random_state = seed, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9usATBLbG4Uv"
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "clfs = []\n",
    "valid_profit = np.zeros(num_folds) \n",
    "preds_test   = np.zeros(test.shape[0])\n",
    "preds_oof    = np.zeros(train.shape[0])\n",
    "importances  = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHjOpbmYG4Ux"
   },
   "outputs": [],
   "source": [
    "# SMOTE settings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 23, n_jobs = 10, sampling_strategy = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnYbk3yBG4Uz"
   },
   "source": [
    "### CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52906,
     "status": "ok",
     "timestamp": 1557933151848,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg",
      "userId": "01253993997636551956"
     },
     "user_tz": -120
    },
    "id": "XdVpCrqxG4U0",
    "outputId": "d2b165a7-c56b-44c0-b62b-466147db5e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom early stopping: select the best out of 600 iterations...\n",
      "[200]\ttraining's binary_logloss: 0.0057359\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0152655\tvalid_1's profit: 95\n",
      "[400]\ttraining's binary_logloss: 0.00468115\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0139562\tvalid_1's profit: 95\n",
      "[600]\ttraining's binary_logloss: 0.00461785\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0138809\tvalid_1's profit: 95\n",
      "Best iteration is:\n",
      "[29]   valid_1 profit: 95; log_loss =  0.043356\n",
      "--------------------------------\n",
      "FOLD 1: PROFIT = 95\n",
      "--------------------------------\n",
      "\n",
      "Custom early stopping: select the best out of 600 iterations...\n",
      "[200]\ttraining's binary_logloss: 0.00525732\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0392173\tvalid_1's profit: 20\n",
      "[400]\ttraining's binary_logloss: 0.00440394\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0406726\tvalid_1's profit: 55\n",
      "[600]\ttraining's binary_logloss: 0.00427252\ttraining's profit: 710\tvalid_1's binary_logloss: 0.041101\tvalid_1's profit: 55\n",
      "Best iteration is:\n",
      "[67]   valid_1 profit: 65; log_loss =  0.039953\n",
      "--------------------------------\n",
      "FOLD 2: PROFIT = 65\n",
      "--------------------------------\n",
      "\n",
      "Custom early stopping: select the best out of 600 iterations...\n",
      "[200]\ttraining's binary_logloss: 0.00555611\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0216629\tvalid_1's profit: 65\n",
      "[400]\ttraining's binary_logloss: 0.00458195\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0231198\tvalid_1's profit: 65\n",
      "[600]\ttraining's binary_logloss: 0.0045153\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0234291\tvalid_1's profit: 65\n",
      "Best iteration is:\n",
      "[41]   valid_1 profit: 75; log_loss =  0.032308\n",
      "--------------------------------\n",
      "FOLD 3: PROFIT = 75\n",
      "--------------------------------\n",
      "\n",
      "Custom early stopping: select the best out of 600 iterations...\n",
      "[200]\ttraining's binary_logloss: 0.00520639\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0374479\tvalid_1's profit: 15\n",
      "[400]\ttraining's binary_logloss: 0.00439227\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0365583\tvalid_1's profit: 15\n",
      "[600]\ttraining's binary_logloss: 0.00430524\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0365163\tvalid_1's profit: 15\n",
      "Best iteration is:\n",
      "[81]   valid_1 profit: 50; log_loss =  0.039262\n",
      "--------------------------------\n",
      "FOLD 4: PROFIT = 50\n",
      "--------------------------------\n",
      "\n",
      "Custom early stopping: select the best out of 600 iterations...\n",
      "[200]\ttraining's binary_logloss: 0.0054151\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0186707\tvalid_1's profit: 90\n",
      "[400]\ttraining's binary_logloss: 0.00445038\ttraining's profit: 710\tvalid_1's binary_logloss: 0.0176266\tvalid_1's profit: 80\n",
      "[600]\ttraining's binary_logloss: 0.00435215\ttraining's profit: 710\tvalid_1's binary_logloss: 0.017582\tvalid_1's profit: 80\n",
      "Best iteration is:\n",
      "[51]   valid_1 profit: 90; log_loss =  0.02964\n",
      "--------------------------------\n",
      "FOLD 5: PROFIT = 90\n",
      "--------------------------------\n",
      "\n",
      "--------------------------------\n",
      "TOTAL PROFIT = 375\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "### CROSS-VALIDATION LOOP\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, y)):\n",
    "    \n",
    "    # data partitioning\n",
    "    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    ## remove outliers\n",
    "    #out_idx = (np.abs(scipy.stats.zscore(trn_x)) < 10).all(axis = 1)\n",
    "    #trn_x = trn_x[out_idx]\n",
    "    #trn_y = trn_y[out_idx]\n",
    "    \n",
    "    # scale data\n",
    "    scaler   = RobustScaler()\n",
    "    trn_x    = pd.DataFrame(scaler.fit_transform(trn_x),      columns = features)\n",
    "    val_x    = pd.DataFrame(scaler.transform(val_x),          columns = features)\n",
    "    tmp_test = pd.DataFrame(scaler.transform(test[features]), columns = features)\n",
    "\n",
    "    # augment training data with SMOTE\n",
    "    trn_x, trn_y = sm.fit_sample(trn_x, trn_y)\n",
    "    trn_x = pd.DataFrame(trn_x, columns = features)\n",
    "    trn_y = pd.Series(trn_y)\n",
    "    \n",
    "    # factor decomposition\n",
    "    tmp_features = copy.deepcopy(features)\n",
    "    #if False:\n",
    "    #    decomp = FactorAnalysis(n_components = 11)\n",
    "    #    decomp.fit(trn_x)\n",
    "    #    trn_x = decomp.transform(trn_x)\n",
    "    #    val_x = decomp.transform(val_x)\n",
    "    #    tmp_test = decomp.transform(tmp_test)\n",
    "    #    tmp_features = ['pc'+str(i) for i in range(decomp.n_components)]\n",
    "    \n",
    "    # add noise to train to reduce overfitting\n",
    "    #trn_x += np.random.normal(0, 0.01, trn_x.shape)\n",
    "    \n",
    "    # mean target encoding\n",
    "    #trn_x, val_x, tmp_test = mean_target_encoding(trn_x, val_x, test, features = ['trustLevel'], target = 'fraud', folds = 5)\n",
    "    #features = [f for f in trn_x.columns if f not in excluded_feats]\n",
    "        \n",
    "    # train lightGBM\n",
    "    print('Custom early stopping: select the best out of %.0f iterations...' % max_rounds)\n",
    "    clf = lgb.LGBMClassifier(**lgb_params) \n",
    "    clf = clf.fit(trn_x, trn_y, \n",
    "                  eval_set              = [(trn_x, trn_y), (val_x, val_y)], \n",
    "                  eval_metric           = prediction_reward, \n",
    "                  #eval_metric           = \"logloss\", \n",
    "                  #early_stopping_rounds = stopping,\n",
    "                  verbose               = verbose)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # predict validation from the best iteration\n",
    "    #best_iter = clf.best_iteration_\n",
    "    best_iter = np.argmax(clf.evals_result_['valid_1']['profit']) + 1\n",
    "    val_preds = clf.predict_proba(val_x, num_iteration = best_iter)[:, 1]\n",
    "    print('Best iteration is:')\n",
    "    print('[' + str(best_iter) + ']   valid_1 profit: ' + str(prediction_reward(val_y, val_preds)[1].astype('int')) + \n",
    "          \"; log_loss = \", str(np.round(log_loss(val_y, val_preds), 6)))\n",
    "       \n",
    "    \n",
    "############ PERFORM 1 ITERATRION OF SELF-TRAINING\n",
    "#\n",
    "#    # predict unlabeled data\n",
    "#    tmp_preds_test = clf.predict_proba(tmp_test, num_iteration = best_iter)[:, 1]\n",
    "#\n",
    "#    # extract most confident preds\n",
    "#    perc_ones  = 0.0001\n",
    "#    perc_zeros = 0.0001\n",
    "#    pmax = np.quantile(tmp_preds_test, 1 - perc_ones)\n",
    "#    pmin = np.quantile(tmp_preds_test, perc_zeros)\n",
    "#    tmp_preds_test[tmp_preds_test >= pmax] = 1\n",
    "#    tmp_preds_test[tmp_preds_test <= pmin] = 0\n",
    "#    confident_ones  = np.where(tmp_preds_test == 1)\n",
    "#    confident_zeros = np.where(tmp_preds_test == 0)\n",
    "#    confident_ones  = confident_ones[0][0:np.round(len(tmp_test)  * perc_ones).astype('int')]\n",
    "#    confident_zeros = confident_zeros[0][0:np.round(len(tmp_test) * perc_zeros).astype('int')]\n",
    "#    confident_idx   = np.concatenate((confident_ones, confident_zeros))\n",
    "#\n",
    "#    # append new data to train\n",
    "#    test_X = tmp_test.iloc[confident_idx]\n",
    "#    test_y = pd.Series(tmp_preds_test[confident_idx])\n",
    "#    trn_x  = trn_x.append(test_X)\n",
    "#    trn_y  = trn_y.append(test_y)\n",
    "#    print('--------------------------------')\n",
    "#    print('Added %.0f cases to training data...' % len(test_y))\n",
    "#    print('--------------------------------')\n",
    "#\n",
    "#    # retrain lightGBM\n",
    "#    print('Custom early stopping: select the best out of %.0f iterations...' % max_rounds)\n",
    "#    clf = lgb.LGBMClassifier(**lgb_params) \n",
    "#    clf = clf.fit(trn_x, trn_y, \n",
    "#                  eval_set              = [(trn_x, trn_y), (val_x, val_y)], \n",
    "#                  eval_metric           = prediction_reward, \n",
    "#                  verbose               = verbose)\n",
    "#\n",
    "#    # find the best iteration\n",
    "#    best_iter = np.argmax(clf.evals_result_['valid_1']['profit']) + 1\n",
    "#    print('Best iteration is:')\n",
    "#    print('[' + str(best_iter) + ']   valid_1 profit: ' + \n",
    "#          str(prediction_reward(val_y, clf.predict_proba(val_x, num_iteration = best_iter)[:, 1])[1].astype('int')) + \n",
    "#          \"; log_loss = \", str(np.round(log_loss(val_y, clf.predict_proba(val_x, num_iteration = best_iter)[:, 1]), 6)))\n",
    "#    \n",
    "############ PERFORM 1 ITERATRION OF SELF-TRAINING\n",
    "\n",
    "\n",
    "    # save predictions\n",
    "    preds_oof[val_idx]    = clf.predict_proba(val_x, num_iteration = best_iter)[:, 1]\n",
    "    valid_profit[n_fold]  = prediction_reward(val_y, preds_oof[val_idx])[1]\n",
    "    preds_test           += clf.predict_proba(tmp_test, num_iteration = best_iter)[:, 1] / folds.n_splits \n",
    "\n",
    "    ## importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df['Feature'] = tmp_features\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold'] = n_fold + 1\n",
    "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # print performance\n",
    "    print('--------------------------------')\n",
    "    print('FOLD%2d: PROFIT = %.0f' % (n_fold + 1, valid_profit[n_fold]))\n",
    "    print('--------------------------------')\n",
    "    print('')\n",
    "        \n",
    "    # clear memory\n",
    "    del trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()\n",
    "    \n",
    "    # uncomment for mean target encoding\n",
    "    #features = [f for f in train.columns if f not in excluded_feats]\n",
    "    \n",
    "    \n",
    "# print overall performance    \n",
    "cv_perf = np.sum(valid_profit)\n",
    "print('--------------------------------')\n",
    "print('TOTAL PROFIT = %.0f' % cv_perf)\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRJGFSmyG4U3"
   },
   "outputs": [],
   "source": [
    "##### RECHECK PROFIT  \n",
    "prediction_reward(y, preds_oof)\n",
    "\n",
    "\n",
    "###### TRACKING RESULTS (5 folds, strat = True, seed = 23)\n",
    "\n",
    "# V1: lgb, 5 folds, default features:   80\n",
    "# V2: add feature:  no. total items:   250\n",
    "# V3: use logloss for ES, not profit:  260\n",
    "# V4: add feature: no. weird actions:  275\n",
    "# V5: custom earlystop for profit:     320\n",
    "# V6: add SMOTE for minority class:    335\n",
    "# V7: add robust data scaling:         350 = 95 + 55 + 75 + 35 + 90\n",
    "# V8: increase learning rate to 0.1:   375 = 95 + 65 + 75 + 50 + 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYJPn6jmG4U6"
   },
   "outputs": [],
   "source": [
    "##### VARIABLE IMPORTANCE\n",
    "\n",
    "# load importance    \n",
    "top_feats = 100\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "    \n",
    "# plot variable importance\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False))\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot as pdf\n",
    "plt.savefig('../var_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbRYVd-iG4U_"
   },
   "source": [
    "### CUTOFF OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RL3esc0gG4VB"
   },
   "outputs": [],
   "source": [
    "##### OPTIMIZE CUTOFF\n",
    "\n",
    "# set step\n",
    "step = 100\n",
    "\n",
    "# search\n",
    "cutoffs = []\n",
    "profits = []\n",
    "for i in range(0, step):\n",
    "    cutoffs.append(i / step)\n",
    "    profits.append(recompute_reward(y, preds_oof, cutoff = cutoffs[i]))\n",
    "        \n",
    "# results\n",
    "plt.figure(figsize = (10,4))\n",
    "sns.lineplot(x = cutoffs[10:step], y = profits[10:step], color = 'red')\n",
    "plt.tight_layout()\n",
    "plt.axvline(x = cutoffs[np.argmax(profits)], color = 'white', linestyle = '--')\n",
    "print('- optimal cutoff = %.4f' % cutoffs[np.argmax(profits)])\n",
    "print('- optimal profit = %.4f' % profits[np.argmax(profits)])\n",
    "plt.savefig('../cutoff_selection.pdf')\n",
    "\n",
    "# update performance\n",
    "cv_perf = recompute_reward(y, preds_oof, cutoff = cutoffs[np.argmax(profits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dsi_jeGG4VE"
   },
   "source": [
    "# 4. SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Tt_FxCwG4VF"
   },
   "outputs": [],
   "source": [
    "# file name\n",
    "model = 'lgb_v8'\n",
    "perf  = str(round(cv_perf, 0).astype('int'))\n",
    "name  = model + '_' + perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gukkBbc9G4VH"
   },
   "outputs": [],
   "source": [
    "# export OOF preds\n",
    "oof = pd.DataFrame({'id': train['id'], 'fraud': preds_oof})\n",
    "oof.to_csv('../oof_preds/' + str(name) + '.csv', index = False)\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cJ8GWH7G4VJ"
   },
   "outputs": [],
   "source": [
    "# check submission\n",
    "sub = pd.DataFrame({'id': test['id'], 'fraud': preds_test})\n",
    "sub['fraud'][sub['fraud'] >  cutoffs[np.argmax(profits)]] = 1\n",
    "sub['fraud'][sub['fraud'] <= cutoffs[np.argmax(profits)]] = 0\n",
    "sub['fraud'] = sub['fraud'].astype('int')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDcmUKk4G4VM"
   },
   "outputs": [],
   "source": [
    "# export submission\n",
    "sub = sub[['fraud']]\n",
    "sub.to_csv('../submissions/' + str(name) + '.csv', index = False)\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSlPhL4dG4VO"
   },
   "outputs": [],
   "source": [
    "# check correlation with previous submission\n",
    "prev_sub = pd.read_csv('../submissions/lgb_v8_375.csv')\n",
    "cor = np.sum(prev_sub[target] == sub.reset_index()[target]) / len(sub)\n",
    "print(\"Share of the same predictions: \" + str(np.round(cor, 6)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "code_2_lgb_main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
